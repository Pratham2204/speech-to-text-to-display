{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to translate\n",
    "# speech to text and text to speech\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "import pyttsx3 \n",
    "\n",
    "# Initialize the recognizer \n",
    "r = sr.Recognizer() \n",
    "\n",
    "# Function to convert text to\n",
    "# speech\n",
    "def SpeakText(command):\n",
    "\t\n",
    "\t# Initialize the engine\n",
    "\tengine = pyttsx3.init()\n",
    "\tengine.say(command) \n",
    "\tengine.runAndWait()\n",
    "\t\n",
    "\t\n",
    "# Loop infinitely for user to\n",
    "# speak\n",
    "\n",
    "while(1): \n",
    "    while(input()):    \n",
    "        # Exception handling to handle\n",
    "        # exceptions at the runtime\n",
    "        try:\n",
    "            \n",
    "            # use the microphone as source for input.\n",
    "            with sr.Microphone() as source2:\n",
    "                \n",
    "                # wait for a second to let the recognizer\n",
    "                # adjust the energy threshold based on\n",
    "                # the surrounding noise level \n",
    "                r.adjust_for_ambient_noise(source2, duration=0.2)\n",
    "                \n",
    "                #listens for the user's input \n",
    "                audio2 = r.listen(source2)\n",
    "                \n",
    "                # Using google to recognize audio\n",
    "                MyText = r.recognize_google(audio2)\n",
    "                MyText = MyText.lower()\n",
    "\n",
    "                print(\"Did you say \",MyText)\n",
    "\n",
    "                with open(r'C:\\Users\\Pratham jain\\Desktop\\RIT\\rit\\semister 8\\major project\\speech-to-text\\op.txt', 'a') as f:\n",
    "                    f.write(MyText + '\\n')\n",
    "                f.close()\n",
    "\n",
    "\n",
    "                # SpeakText(MyText)\n",
    "                \n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results; {0}\".format(e))\n",
    "            \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"error occurred\")\n",
    "\n",
    "        \n",
    "\n",
    "#first code for main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import time\n",
    "import keyboard\n",
    "import pyaudio\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def SpeakText(command):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(command)\n",
    "    engine.runAndWait()\n",
    "\n",
    "file = None\n",
    "try:\n",
    "    file = open(\"op.txt\", \"w\")\n",
    "except Exception as e:\n",
    "    print(f\"Error opening file: {e}\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    if keyboard.is_pressed('space'):\n",
    "        with sr.Microphone() as source2:\n",
    "            r.adjust_for_ambient_noise(source2, duration=0.2)\n",
    "            try:\n",
    "                audio2 = r.listen(source2)\n",
    "                MyText = r.recognize_google(audio2).lower()\n",
    "                print(\"Did you say \", MyText)\n",
    "                file.write(MyText + \"\\n\")\n",
    "                file.flush()\n",
    "                SpeakText(MyText)\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Could not request results; {0}\".format(e))\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"error occurred\")\n",
    "    elif keyboard.is_pressed('escape'):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new code for improvement\n",
    "\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "\n",
    "# Download the NLTK tagger if necessary\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define the list of keywords to extract\n",
    "keyword_list = [\"eyebrows\", \"eyes\", \"hair\", \"head\", \"lips\", \"mustache\", \"nose\"]\n",
    "\n",
    "# Define the path to the image folder\n",
    "image_folder = \"path/to/image/folder\"\n",
    "\n",
    "# Open the \"op.txt\" file and read the latest line\n",
    "with open(\"op.txt\", \"r\") as file:\n",
    "    line = file.readlines()[-1].strip()\n",
    "\n",
    "# Define a regular expression to match adjectives\n",
    "adj_regex = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(adj) for adj in adjective_list) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "# Tokenize the sentence into words and tags\n",
    "tagged_words = nltk.pos_tag(word_tokenize(line))\n",
    "\n",
    "# Initialize empty lists for adjectives and keywords\n",
    "adjectives = []\n",
    "keywords = []\n",
    "\n",
    "# Loop through each word and tag\n",
    "for i, (word, tag) in enumerate(tagged_words):\n",
    "    # Check if the word is a keyword\n",
    "    if word in keyword_list:\n",
    "        # Check if there is an adjective before this keyword\n",
    "        if i > 0 and tagged_words[i-1][1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "            # Add the adjective and keyword to their respective lists\n",
    "            adjectives.append(tagged_words[i-1][0])\n",
    "            keywords.append(word)\n",
    "\n",
    "# Combine the adjectives and keywords into phrases\n",
    "phrases = [\" \".join(adj_keyword) for adj, keyword in zip(adjectives, keywords) if keyword]\n",
    "\n",
    "# Loop through each phrase and display the corresponding image\n",
    "for phrase in phrases:\n",
    "    for file in os.listdir(image_folder):\n",
    "        if phrase.lower() in file.lower():\n",
    "            image_path = os.path.join(image_folder, file)\n",
    "            Image.open(image_path).show()\n",
    "            break\n",
    "\n",
    "#In this code, we first define the list of keywords to extract. We then download the NLTK tagger if necessary and tokenize the sentence into words and tags using the pos_tag function.\n",
    "\n",
    "#Next, we initialize empty lists for adjectives and keywords. We then loop through each word and tag, and check if the word is a keyword. If the word is a keyword, we check if there is an adjective before this keyword. If there is, we add the adjective and keyword to their respective lists.\n",
    "\n",
    "#After that, we combine the adjectives and keywords into phrases, just as in the previous code.\n",
    "\n",
    "#Finally, we define the path to the image folder and loop through each phrase. For each phrase, we search for the corresponding image file in the image folder and display the image using the PIL library.\n",
    "\n",
    "#Note that this code extracts adjective-keyword pairs only if there is a keyword following the adjective. This means that adjectives that do not modify a keyword will be ignored, as in the example sentence \"That beautiful girl has very short hair\". The adjective \"beautiful\" does not modify any of the keywords, so it is ignored, and only the adjective-keyword pair \"short hair\" is extracted and used to search for the corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "\n",
    "# Download the NLTK tagger if necessary\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define the list of keywords to extract\n",
    "keyword_list = [\"eyebrows\", \"eyes\", \"hair\", \"head\", \"lips\", \"mustache\", \"nose\"]\n",
    "\n",
    "# Define the path to the image folder\n",
    "image_folder = \"path/to/image/folder\"\n",
    "\n",
    "# Open the \"op.txt\" file and read the latest line\n",
    "with open(\"op.txt\", \"r\") as file:\n",
    "    line = file.readlines()[-1].strip()\n",
    "\n",
    "# Define a regular expression to match adjectives\n",
    "adj_regex = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(adj) for adj in adjective_list) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "# Tokenize the sentence into words and tags\n",
    "tagged_words = nltk.pos_tag(word_tokenize(line))\n",
    "\n",
    "# Initialize empty lists for adjectives and keywords\n",
    "adjectives = []\n",
    "keywords = []\n",
    "\n",
    "# Loop through each word and tag\n",
    "for i, (word, tag) in enumerate(tagged_words):\n",
    "    # Check if the word is a keyword\n",
    "    if word in keyword_list:\n",
    "        # Check if there is an adjective before this keyword\n",
    "        if i > 0 and tagged_words[i-1][1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "            # Add the adjective and keyword to their respective lists\n",
    "            adjectives.append(tagged_words[i-1][0])\n",
    "            keywords.append(word)\n",
    "\n",
    "# Combine the adjectives and keywords into phrases\n",
    "phrases = [\" \".join(adj_keyword) for adj, keyword in zip(adjectives, keywords) if keyword]\n",
    "\n",
    "# Loop through each phrase and display the corresponding image\n",
    "for phrase in phrases:\n",
    "    # Get the keyword from the phrase\n",
    "    keyword = phrase.split()[-1]\n",
    "    # Define the path to the subfolder\n",
    "    subfolder = os.path.join(image_folder, keyword)\n",
    "    # Search for the image file in the subfolder\n",
    "    for file in os.listdir(subfolder):\n",
    "        if phrase.lower() in file.lower():\n",
    "            # Combine the subfolder path and the image file name\n",
    "            image_path = os.path.join(subfolder, file)\n",
    "            # Display the image\n",
    "            Image.open(image_path).show()\n",
    "            # Exit the loop since we found the image\n",
    "            break\n",
    "    else:\n",
    "        print(f\"No image found for phrase '{phrase}'\")\n",
    "\n",
    "#In this code, we first define the list of keywords to extract. We then download the NLTK tagger if necessary and tokenize the sentence into words and tags using the pos_tag function.\n",
    "\n",
    "#Next, we initialize empty lists for adjectives and keywords. We then loop through each word and tag, and check if the word is a keyword. If the word is a keyword, we check if there is an adjective before this keyword. If there is, we add the adjective and keyword to their respective lists.\n",
    "\n",
    "#After that, we combine the adjectives and keywords into phrases, just as in the previous code.\n",
    "\n",
    "#Finally, we define the path to the image folder and loop through each phrase. For each phrase, we get the keyword from the phrase and define the path to the subfolder. We then search for the image file in the subfolder using the os.listdir function. If the image file is found, we combine the subfolder path and the image file name and display the image using the PIL library. If the image file is not found, we print a message indicating that no image was found for that phrase.\n",
    "\n",
    "#Note that this code searches for the image in the subfolder named after the keyword. If the extracted phrase is \"short hair\", it will look for the image in the \"hair\" subfolder. Similarly, if the extracted phrase is \"round head\", it will look for the image in the \"head\" subfolder. If the image is not found in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "# Download the NLTK tagger if necessary\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define the list of keywords to extract\n",
    "keyword_list = [\"eyebrows\", \"eyes\", \"hair\", \"head\", \"lips\", \"mustache\", \"nose\"]\n",
    "\n",
    "# Define the path to the image folder\n",
    "image_folder = \"path/to/image/folder\"\n",
    "\n",
    "# Define the path to the feedback file\n",
    "feedback_file = \"path/to/feedback.txt\"\n",
    "\n",
    "# Open the \"op.txt\" file and read the latest line\n",
    "with open(\"op.txt\", \"r\") as file:\n",
    "    line = file.readlines()[-1].strip()\n",
    "\n",
    "# Define a regular expression to match adjectives\n",
    "adj_regex = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(adj) for adj in adjective_list) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "# Tokenize the sentence into words and tags\n",
    "tagged_words = nltk.pos_tag(word_tokenize(line))\n",
    "\n",
    "# Initialize empty lists for adjectives and keywords\n",
    "adjectives = []\n",
    "keywords = []\n",
    "\n",
    "# Loop through each word and tag\n",
    "for i, (word, tag) in enumerate(tagged_words):\n",
    "    # Check if the word is a keyword\n",
    "    if word in keyword_list:\n",
    "        # Check if there is an adjective before this keyword\n",
    "        if i > 0 and tagged_words[i-1][1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "            # Add the adjective and keyword to their respective lists\n",
    "            adjectives.append(tagged_words[i-1][0])\n",
    "            keywords.append(word)\n",
    "\n",
    "# Combine the adjectives and keywords into phrases\n",
    "phrases = [\" \".join(adj_keyword) for adj, keyword in zip(adjectives, keywords) if keyword]\n",
    "\n",
    "# Loop through each phrase and display the corresponding image\n",
    "for phrase in phrases:\n",
    "    # Get the keyword from the phrase\n",
    "    keyword = phrase.split()[-1]\n",
    "    # Define the path to the subfolder\n",
    "    subfolder = os.path.join(image_folder, keyword)\n",
    "    # Search for the image file in the subfolder\n",
    "    for file in os.listdir(subfolder):\n",
    "        if phrase.lower() in file.lower():\n",
    "            # Combine the subfolder path and the image file name\n",
    "            image_path = os.path.join(subfolder, file)\n",
    "            # Display the image\n",
    "            Image.open(image_path).show()\n",
    "            # Exit the loop since we found the image\n",
    "            break\n",
    "    else:\n",
    "        # Image not found, append the phrase to the feedback file\n",
    "        with open(feedback_file, \"a\") as file:\n",
    "            file.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {phrase} (image not found in {subfolder})\\n\")\n",
    "\n",
    "\n",
    "#lets improve the code in more specific way, lets say if the sentence say \"that beautiful girl has short hair and sharp eyes\" the extracted words would be \"short hair\" and \"sharp eyes\" which need to be displayed but if in the eyes subfolder there is no \"sharp eyes\" image then the extracted word whose image is not found would be appended to the feedback.txt file along with the sentence it was extracted from with details such as date, there everytime a image is not found those extracted words will be stored in feedback.txt\n",
    "#In this code, we first define the list of keywords to extract. We then download the NLTK tagger if necessary and tokenize the sentence into words and tags using the pos_tag function.\n",
    "\n",
    "#Next, we initialize empty lists for adjectives and keywords. We then loop through each word and tag, and check if the word is a keyword. If the word is a keyword, we check if there is an adjective before this keyword. If there is, we add the adjective and keyword to their respective lists.\n",
    "\n",
    "#After that, we combine the adjectives and keywords into phrases, just as in the previous code.\n",
    "\n",
    "#Finally, we define the path to the image folder and loop through each phrase. For each phrase, we get the keyword from the phrase and define the path to the subfolder. We then search for the image file in the subfolder using the os.listdir function. If the image file is found, we combine the subfolder path and the image file name and display the image using the PIL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#Yes, you can use the watchdog library to monitor the op.txt file for modifications and execute the code automatically when the file is modified. Here's an example of how to modify the code to do that:\n",
    "\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import time\n",
    "import sys\n",
    "import watchdog.observers\n",
    "import watchdog.events\n",
    "\n",
    "# Download the NLTK tagger if necessary\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define the list of keywords to extract\n",
    "keyword_list = [\"eyebrows\", \"eyes\", \"hair\", \"head\", \"lips\", \"mustache\", \"nose\"]\n",
    "\n",
    "# Define the path to the image folder\n",
    "image_folder = \"path/to/image/folder\"\n",
    "\n",
    "# Define the path to the feedback file\n",
    "feedback_file = \"path/to/feedback.txt\"\n",
    "\n",
    "# Define a regular expression to match adjectives\n",
    "adj_regex = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(adj) for adj in adjective_list) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "# Define the event handler for file modifications\n",
    "class FileEventHandler(watchdog.events.FileSystemEventHandler):\n",
    "    def on_modified(self, event):\n",
    "        if event.src_path == \"op.txt\":\n",
    "            # Execute the code when op.txt is modified\n",
    "            self.execute_code()\n",
    "\n",
    "    def execute_code(self):\n",
    "        # Tokenize the sentence into words and tags\n",
    "        with open(\"op.txt\", \"r\") as file:\n",
    "            line = file.readlines()[-1].strip()\n",
    "        tagged_words = nltk.pos_tag(word_tokenize(line))\n",
    "\n",
    "        # Initialize empty lists for adjectives and keywords\n",
    "        adjectives = []\n",
    "        keywords = []\n",
    "\n",
    "        # Loop through each word and tag\n",
    "        for i, (word, tag) in enumerate(tagged_words):\n",
    "            # Check if the word is a keyword\n",
    "            if word in keyword_list:\n",
    "                # Check if there is an adjective before this keyword\n",
    "                if i > 0 and tagged_words[i-1][1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "                    # Add the adjective and keyword to their respective lists\n",
    "                    adjectives.append(tagged_words[i-1][0])\n",
    "                    keywords.append(word)\n",
    "\n",
    "        # Combine the adjectives and keywords into phrases\n",
    "        phrases = [\" \".join(adj_keyword) for adj, keyword in zip(adjectives, keywords) if keyword]\n",
    "\n",
    "        # Loop through each phrase and display the corresponding image\n",
    "        for phrase in phrases:\n",
    "            # Get the keyword from the phrase\n",
    "            keyword = phrase.split()[-1]\n",
    "            # Define the path to the subfolder\n",
    "            subfolder = os.path.join(image_folder, keyword)\n",
    "            # Search for the image file in the subfolder\n",
    "            for file in os.listdir(subfolder):\n",
    "                if phrase.lower() in file.lower():\n",
    "                    # Combine the subfolder path and the image file name\n",
    "                    image_path = os.path.join(subfolder, file)\n",
    "                    # Display the image\n",
    "                    Image.open(image_path).show()\n",
    "                    # Exit the loop since we found the image\n",
    "                    break\n",
    "            else:\n",
    "                # Image not found, append the phrase to the feedback file\n",
    "                with open(feedback_file, \"a\") as file:\n",
    "                    file.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {phrase} (image not found in {subfolder})\\n\")\n",
    "\n",
    "# Create the observer\n",
    "observer = watchdog.observers.Observer()\n",
    "\n",
    "# Schedule the event handler for file modifications\n",
    "event_handler = FileEventHandler()\n",
    "observer.schedule(event_handler, path=\".\", recursive=False)\n",
    "\n",
    "# Start the observer\n",
    "observer.start()\n",
    "\n",
    "# Run the code indefinitely\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    observer.stop()\n",
    "observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each phrase and display the corresponding image\n",
    "for phrase in phrases:\n",
    "    # Get the keyword from the phrase\n",
    "    keyword = phrase.split()[-1]\n",
    "    # Define the path to the subfolder\n",
    "    subfolder = os.path.join(image_folder, keyword)\n",
    "    # Search for the image file in the subfolder\n",
    "    for file in os.listdir(subfolder):\n",
    "        if phrase.lower() in file.lower():\n",
    "            # Combine the subfolder path and the image file name\n",
    "            image_path = os.path.join(subfolder, file)\n",
    "            # Display the image\n",
    "            Image.open(image_path).show()\n",
    "            # Exit the loop since we found the image\n",
    "            break\n",
    "    else:\n",
    "        # Image not found, append the phrase to the feedback file\n",
    "        with open(feedback_file, \"a\") as file:\n",
    "            file.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {phrase} (image not found in {subfolder})\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
